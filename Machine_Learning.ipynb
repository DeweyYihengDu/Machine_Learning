{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Flatten layer to flatten the input image tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # A stack of linear layers with ReLU activation\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # Linear layer with input size of 28*28 and output size of 512\n",
    "            nn.Linear(28*28, 512),\n",
    "            # ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Linear layer with input size of 512 and output size of 512\n",
    "            nn.Linear(512, 512),\n",
    "            # ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Linear layer with input size of 512 and output size of 10\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image tensor\n",
    "        x = self.flatten(x)\n",
    "        # Pass the tensor through the stack of linear layers with ReLU activation\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # Return the output logits\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Pytorch implementation of a neural network class. The NeuralNetwork class inherits from Pytorch's nn.Module class, which is a base class for all neural network modules in Pytorch.\n",
    "\n",
    "The __init__ method is used to define the architecture of the neural network. It creates a flatten layer, which is used to flatten the input image tensor. Then it creates a linear_relu_stack attribute, which is a sequential container for a stack of linear layers with ReLU activation. The stack of linear layers with ReLU activation contains three linear layers, where the input size of the first linear layer is 28*28, the output size is 512, the input size of second linear layer is 512, the output size is 512, the input size of third linear layer is 512, the output size is 10.\n",
    "\n",
    "The forward method is used to define the forward pass of the neural network. It takes an input tensor x, flattens it using the flatten layer, and then passes it through the stack of linear layers with ReLU activation. The output of this forward pass is the logits, which are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1219,  0.1985,  0.1922, -0.1765,  0.2572,  0.2387,  0.2010,  0.8394,\n",
      "          0.0188,  0.0038, -0.2983,  0.3365, -0.3779,  0.2824, -0.0703,  0.1712,\n",
      "         -0.1543, -0.3543, -0.2228,  0.3906],\n",
      "        [ 0.0446, -0.0163,  0.1951, -0.4288, -0.2312, -0.0752,  0.4822,  0.4860,\n",
      "         -0.3369, -0.2269,  0.1280,  0.1833,  0.0320,  0.1048,  0.3237, -0.3083,\n",
      "         -0.0826, -0.6168,  0.1285,  0.1423],\n",
      "        [ 0.0136,  0.1525,  0.4633, -0.3016, -0.2295,  0.1458,  0.1963,  0.4898,\n",
      "          0.2216, -0.2207,  0.1373,  0.0810,  0.0977,  0.3156,  0.0848, -0.0605,\n",
      "         -0.5306,  0.0651,  0.3781, -0.1921]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.1985, 0.1922, 0.0000, 0.2572, 0.2387, 0.2010, 0.8394, 0.0188,\n",
      "         0.0038, 0.0000, 0.3365, 0.0000, 0.2824, 0.0000, 0.1712, 0.0000, 0.0000,\n",
      "         0.0000, 0.3906],\n",
      "        [0.0446, 0.0000, 0.1951, 0.0000, 0.0000, 0.0000, 0.4822, 0.4860, 0.0000,\n",
      "         0.0000, 0.1280, 0.1833, 0.0320, 0.1048, 0.3237, 0.0000, 0.0000, 0.0000,\n",
      "         0.1285, 0.1423],\n",
      "        [0.0136, 0.1525, 0.4633, 0.0000, 0.0000, 0.1458, 0.1963, 0.4898, 0.2216,\n",
      "         0.0000, 0.1373, 0.0810, 0.0977, 0.3156, 0.0848, 0.0000, 0.0000, 0.0651,\n",
      "         0.3781, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[0.0000, 0.1985, 0.1922, 0.0000, 0.2572, 0.2387, 0.2010, 0.8394, 0.0188,\n",
      "         0.0038, 0.0000, 0.3365, 0.0000, 0.2824, 0.0000, 0.1712, 0.0000, 0.0000,\n",
      "         0.0000, 0.3906],\n",
      "        [0.0446, 0.0000, 0.1951, 0.0000, 0.0000, 0.0000, 0.4822, 0.4860, 0.0000,\n",
      "         0.0000, 0.1280, 0.1833, 0.0320, 0.1048, 0.3237, 0.0000, 0.0000, 0.0000,\n",
      "         0.1285, 0.1423],\n",
      "        [0.0136, 0.1525, 0.4633, 0.0000, 0.0000, 0.1458, 0.1963, 0.4898, 0.2216,\n",
      "         0.0000, 0.1373, 0.0810, 0.0977, 0.3156, 0.0848, 0.0000, 0.0000, 0.0651,\n",
      "         0.3781, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.1985, 0.1922, 0.0000, 0.2572, 0.2387, 0.2010, 0.8394, 0.0188,\n",
      "         0.0038, 0.0000, 0.3365, 0.0000, 0.2824, 0.0000, 0.1712, 0.0000, 0.0000,\n",
      "         0.0000, 0.3906],\n",
      "        [0.0446, 0.0000, 0.1951, 0.0000, 0.0000, 0.0000, 0.4822, 0.4860, 0.0000,\n",
      "         0.0000, 0.1280, 0.1833, 0.0320, 0.1048, 0.3237, 0.0000, 0.0000, 0.0000,\n",
      "         0.1285, 0.1423],\n",
      "        [0.0136, 0.1525, 0.4633, 0.0000, 0.0000, 0.1458, 0.1963, 0.4898, 0.2216,\n",
      "         0.0000, 0.1373, 0.0810, 0.0977, 0.3156, 0.0848, 0.0000, 0.0000, 0.0651,\n",
      "         0.3781, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0276, -0.0036,  0.0230,  ..., -0.0172, -0.0309, -0.0188],\n",
      "        [ 0.0296,  0.0253,  0.0308,  ..., -0.0354,  0.0219, -0.0259]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0013, 0.0119], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0384,  0.0064,  0.0418,  ...,  0.0289,  0.0019,  0.0320],\n",
      "        [-0.0194,  0.0168,  0.0332,  ...,  0.0417,  0.0362,  0.0055]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0284, 0.0169], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0397,  0.0030, -0.0016,  ..., -0.0117, -0.0053, -0.0244],\n",
      "        [ 0.0132, -0.0379, -0.0373,  ...,  0.0293, -0.0429,  0.0435]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0077, 0.0317], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6be8fc375e7f2a12d966ca69c4c87dfa0a5e21291543001a29f6d3fc266e190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
